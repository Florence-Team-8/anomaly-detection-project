{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1583e8ae-4797-4a0d-ad5d-bb864181a85f",
   "metadata": {},
   "source": [
    "## Project prep\n",
    "1. Which lesson appears to attract the most traffic consistently across cohorts (per program)?\n",
    "    - cohort pulled from the sql table \n",
    "    - sort all the endpoints \n",
    "2. Is there a cohort that referred to a lesson significantly more than other cohorts seemed to gloss over?\n",
    "    - divided web dev and data science (4 different programs)\n",
    "    - sort all the endpoints \n",
    "    - counts of cohorts accessing same endpoints\n",
    "3. Are there students who, when active, hardly access the curriculum? If so, what information do you have about these students?\n",
    "    - active status indicator \n",
    "    - filter by the student being active \n",
    "4. Is there any suspicious activity, such as users/machines/etc accessing the curriculum who shouldnâ€™t be? Does it appear that any web-scraping is happening? Are there any suspicious IP addresses?\n",
    "    - figure out what constitutes a suspicious IP address\n",
    "        - any ip addresses from china?\n",
    "        - figure out general area of ip address\n",
    "    - how to recognize web scraping\n",
    "    - SKIP \n",
    "5. At some point in 2019, the ability for students and alumni to access both curriculums (web dev to ds, ds to web dev) should have been shut off. Do you see any evidence of that happening? Did it happen before?\n",
    "    - how to classify the endpoints as web dev and not this might be crazy\n",
    "6. What topics are grads continuing to reference after graduation and into their jobs (for each program)?\n",
    "    - active status indicator \n",
    "    - each row of a ping would have student_active column - Parker \n",
    "    - \n",
    "7. Which lessons are least accessed?\n",
    "    - kind of related to 2\n",
    "8. Anything else I should be aware of?\n",
    "\n",
    "1 is related to 7 and 2 \n",
    "\n",
    "3 is relate to 6\n",
    "\n",
    "anything extra we find\n",
    "\n",
    "### Data Frame Must Haves\n",
    "- log data\n",
    "- cohort data\n",
    "- Join on cohort id (Left Join) - Lupe has the join and the drop columns \n",
    "- column that has the active student at time of request (yes or no)\n",
    "- need key for all the program ids (is_data_science maybe) \n",
    "- endpoint as a web dev, or data science \n",
    "    - Anything came before ada's start date is webdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bde888-9006-461d-abe1-d3ba2b121971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare file for anomaly detection exercises\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Vis tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "# defining some functions to make it easier. will go in Wrangle function\n",
    "from env import host, password, user\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c7c199-0c08-4301-bfa8-5a8bd2b3f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Getting database Url ################\n",
    "def get_db_url(db_name, user=user, host=host, password=password):\n",
    "    \"\"\"\n",
    "        This helper function takes as default the user host and password from the env file.\n",
    "        You must input the database name. It returns the appropriate URL to use in connecting to a database.\n",
    "    \"\"\"\n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/{db_name}'\n",
    "    return url\n",
    "\n",
    "######################### get generic data #########################\n",
    "def get_any_data(database, sql_query):\n",
    "    '''\n",
    "    put in the query and the database and get the data you need in a dataframe\n",
    "    '''\n",
    "\n",
    "    return pd.read_sql(sql_query, get_db_url(database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418e8a6-a625-4141-85a5-bef0e137feed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d01146-d6cf-46da-83c0-78d3a16e5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_curriculum_logs(csv_name = \"anonymized-curriculum-access-07-2021.txt\"):\n",
    "    '''\n",
    "    This function reads the csv of curriculum access logs\n",
    "    If the file name needs to be changed change the default arguement\n",
    "    '''\n",
    "    # assign column names to use\n",
    "    colnames = ['date', 'endpoint', 'user_id', 'cohort_id', 'source_ip']\n",
    "    # read csv\n",
    "    df = pd.read_csv(csv_name, \n",
    "                 sep=\"\\s\", \n",
    "                 header=None, \n",
    "                 names = colnames, \n",
    "                 usecols=[0, 2, 3, 4, 5])\n",
    "    return df\n",
    "\n",
    "def make_datetime_index(df, col_name):\n",
    "    '''\n",
    "    This function takes in a dataframe \n",
    "    A column name of the column that is your date (as string)\n",
    "    Performs basic to_datetime conversion and sets tha column as the index\n",
    "    '''\n",
    "    \n",
    "    df[col_name] = pd.to_datetime(df[col_name])\n",
    "\n",
    "    df = df.set_index(col_name)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prep(df):\n",
    "    '''\n",
    "    This function takes in the curriculum log data, and the user number you want to look at\n",
    "    returns a dataframe of the pages they accessed and when\n",
    "    '''\n",
    "    #df = df[df.user_id == user]\n",
    "    df = make_datetime_index(df, 'date')\n",
    "    pages = df['endpoint'].resample('d').count()\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd98568-4372-4547-a55e-10cb5ad21314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5789d8fa33d1>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(csv_name,\n"
     ]
    }
   ],
   "source": [
    "df = acquire_curriculum_logs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636e6af5-e77d-4f98-aca4-9f7ce78ee3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbase = 'curriculum_logs'\n",
    "sqlquery = 'SELECT * FROM cohorts'\n",
    "\n",
    "cohorts = get_any_data(dbase, sqlquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9bc973-58c5-4464-acc9-6c56eec10097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db84c9a-24a3-43b0-bc25-b00cf3ae31a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
